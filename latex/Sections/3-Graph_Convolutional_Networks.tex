\section{Graph Convolutional Networks}

The recent popularity of convolutional neural networks \cite{40k}, and their
success in classifying images and other tasks with a striking accuracy, has intrigued the scientific
community with the question of whether -at least a variant of them- can be leveraged in learning tasks
of graph networked data. The use of graph structures in computer
vision\cite{survey}, as well in the representation of social networks
\cite{kleinberg_book}, has made this task look appealing and worth of investigation-research.
However, using convolutional neural networks for
networked data directly, is not straightforward and it poses significant
challenges.\\
\spara{Challenges in Graph Convolutional Networks} First and foremost,
as CNNs were mainly used for image classification, they assume that
data lie on a regular grid in a geometric space (most commonly
Euclidean). On the contrary, graph data are a typical form of unordered data that lie in
an irregular domain. Also, the heavy-tailed distribution of node degrees
in real networks \cite{smth} makes the filtering-convolution part difficult,
as it is not easy to define a constant-sized neighborhood and apply a
localized filter, as in the traditional CNN case. Also, the pooling stage has to
be defined as well.
\spara{Convolution in graphs} First efforts to address the above mentioned issues
by Bruna et al. \cite{Lecun} and introduce the architecture of CNNs to networked data,
mainly draw from the field of Graph Signal Processing \cite{shuman}.
Graphs are generally considered undirected and their representation is given
through their \textbf{Laplacian $L = D -A$} or more commonly the normalized
\textbf{Laplacian $L = I_n - D^{-\frac{1}{2}}AD^{-\frac{1}{2}}$}. As this matrix
is a real symmetric and positive semidefinite, it admits an eigenvector
decomposition $L=U\Lambda U^T$, where $U$ is a square orthonormal matrix with
the eigenvectors as its columns, and $\Lambda$ is the diagonal matrix of eigenvalues.
Letting $x\in \mathbb{R}^{n}$ be a feature vector of the nodes of a graph,
the {\em graph fourier transform} is then defined as $\hat{x}=U^T x \in \mathbb{R}^n$
and its inverse as $x = U\hat{x}$. The \textbf{convolution operator} on a graph
$\mathcal{G}$ is defined on the Fourier domain, as:
\begin{equation*}
x *_{\mathcal{G}} y = U((U^T x)\odot (U^T y))
\end{equation*}
where $\odot$ denotes the element-wise Hadamard product.\\
Therefore, a signal $x$ is filtered by $g_{\theta}$, as:\\
\begin{equation}
y = g_{\theta}(L)x = g_{\theta} (U\Lambda U^T)x = U g_{\theta}(\Lambda ) U^T x
\end{equation}
where $g_{\theta}(\Lambda) = diag(\theta)$ is a non-parametric filter.\\
This approach has, however, the following limitations:
\begin{itemize}
\item [1.] Filters are not localized
\item [2.] Their learning complexity scales with the dimensionality of the data $O(n)$
\item [3.] The computational cost of filtering is high- $O(n^2)$, due to the
multiplication with the Fourier basis $U$.
\end{itemize}

\subsubsection*{ChebNet\cite{defferard}}
Deferrard et al. \cite{defferard} were the first to propose a way to deal with
the above issues.\\
\spara{Localized Filter} First of all, they propose the use of a polynomial filter:\\
\begin{equation}
g_{\theta}(\Lambda ) = \sum_{k=0}^{K-1}\theta_k \Lambda^k
\end{equation}
where the parameter $\theta \in \mathbb{R}^K$ is a vector of polynomial
coefficients. As $(L^K)_{i,j} = 0$ whenever $d_{\mathcal{G}}(i,j)>K$, these
spectral filters are $K$-localized.\\
\spara{Learning complexity} Their learning complexity is equal to the support
size of the filter, hence $O(K)$, as in conventional CNNs.\\
\spara{Computational cost of filtering} As mentioned earlier, because of the
multiplication of the filter with the Fourier basis $U$, the computational cost
is relatively high, $O(n^2)$. For this reason, the express the filter as a
Chebyshev polynomial of order $k$, that can be computed using the recurrence
$T_k(x) = 2xT_{k-1}(x)-T_{k-2}(x)$ with $T_o = 1$ and $T_1 = x$. Thus, the whole
filtering operation can be written as $y = g_{\theta}(L)x =
\sum_{k=0}^{K-1}\theta_k T_k (\widetilde{\Lambda })x$, where $T_k (\Lambda ) \in
\mathbb{R}^{nxn}$ is the Chebyshev polynomial of order $k$ evaluated at the
scaled Laplacian $\widetilde{\Lambda} = 2L/\lambda_{max} - I_n$, using the
recurrence $\bar{x}_k = 2\widetilde{\Lambda}\bar{x}_{k-1} - \bar{x}_{k-2}$, with
$\bar{x}_0 = x$ and $\bar{x}_1 = \widetilde{\Lambda}x$. The cost of the
filtering operation becomes then $O(K|E|)$, as it is a multiplication of a
sparse matrix with a column vector of size $K$.\\
\spara{Graph Coarsening and Pooling}
Pooling on traditional convolutional neural networks, is performed usually on a
patch of neighboring pixels on an image. However, in graphs, we can not explicitly
define this notion of neighborhood as in a Euclidean grid. Therefore, the authors of
this paper propose a multilevel clustering approach, that not only clusters
similar vertices - in a topological fashion- together, but also produces coarser
versions of the graph on each level, resembling closely the pooling operation of traditional
CNNs. The multilevel clustering algorithm chosen is Graclus \cite{Kulis}.\\
In order to perform pooling in a fast and memory efficient manner, nodes are
organized in a balanced binary tree. Specifically, after coarsening, each node
has either two childer, if it was matched at the finer level, or one, if it was
not. Those singleton nodes are paired with fake nodes, having a neutral value
-with respect to the activation function- as its input. On the following figure,
we can see a visualization of this pooling procedure:\\

its input signal.
\subsubsection*{Kipf}

\subsubsection*{GraphSAGE}
