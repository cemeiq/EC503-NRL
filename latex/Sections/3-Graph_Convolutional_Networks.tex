\section{Graph Convolutional Networks}

The recent popularity of convolutional neural networks \cite{40k}, and their
success in classifying images and other tasks with a striking accuracy, has intrigued the scientific
community with the question of whether -at least a variant of them- can be leveraged in learning tasks
of graph networked data. The use of graph structures in computer
vision\cite{survey}, as well in the representation of social networks
\cite{kleinberg_book}, has made this task look appealing and worth of investigation-research.
However, using convolutional neural networks for
networked data directly, is not straightforward and it poses significant
challenges.\\
\spara{Challenges in Graph Convolutional Networks} First and foremost,
as CNNs were mainly used for image classification, they assume that
data lie on a regular grid in a geometric space (most commonly
Euclidean). On the contrary, graph data are a typical form of unordered data that lie in
an irregular domain. Also, the heavy-tailed distribution of node degrees
in real networks \cite{smth} makes the filtering-convolution part difficult,
as it is not easy to define a constant-sized neighborhood and apply a
localized filter, as in the traditional CNN case. Also, the pooling stage has to
be defined as well.
\spara{Convolution in graphs} First efforts to address the above mentioned issues
by Bruna et al. \cite{Lecun} and introduce the architecture of CNNs to networked data,
mainly draw from the field of Graph Signal Processing \cite{shuman}.
Graphs are generally considered undirected and their representation is given
through their \textbf{Laplacian $L = D -A$} or more commonly the normalized
\textbf{Laplacian $L = I_n - D^{-\frac{1}{2}}AD^{-\frac{1}{2}}$}. As this matrix
is a real symmetric and positive semidefinite, it admits an eigenvector
decomposition $L=U\Lambda U^T$, where $U$ is a square orthonormal matrix with
the eigenvectors as its columns, and $\Lambda$ is the diagonal matrix of eigenvalues.
Letting $x\in \mathbb{R}^{n}$ be a feature vector of the nodes of a graph,
the {\em graph fourier transform} is then defined as $\hat{x}=U^T x \in \mathbb{R}^n$
and its inverse as $x = U\hat{x}$. The \textbf{convolution operator} on a graph
$\mathcal{G}$ is defined on the Fourier domain, as:
\begin{equation*}
x *_{\mathcal{G}} y = U((U^T x)\odot (U^T y))
\end{equation*}
where $\odot$ denotes the element-wise Hadamard product.\\
Therefore, a signal $x$ is filtered by $g_{\theta}$, as:\\
\begin{equation}
y = g_{\theta}(L)x = g_{\theta} (U\Lambda U^T)x = U g_{\theta}(\Lambda ) U^T x
\end{equation}
where $g_{\theta}(\Lambda) = diag(\theta)$ is a non-parametric filter.\\
This approach has, however, the following limitations:
\begin{itemize}
\item [1.] Filters are not localized
\item [2.] Their learning complexity scales with the dimensionality of the data $O(n)$
\item [3.] The computational cost of filtering is high- $O(n^2)$, due to the
multiplication with the Fourier basis $U$.
\end{itemize}

\subsubsection*{ChebNet\cite{defferard}}
