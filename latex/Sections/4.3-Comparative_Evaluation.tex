\subsection{Comparative Evaluation}

Unfortunately not all algorithms could be run on all the datasets. MDS and Spectral Embedding
require quadratic space with respect to the number of the nodes, giving them an effective ceiling of
about 10000 nodes. The largest dataset, com-Youtube proved too much for all implementations except
deepwalk. We were unable to run the published code for LINE, depsite our best efforts.

The following table~\ref{tab:results} shows the micro and macro precision, recall and F1 score for
classification in each dataset.

\input{results.tex}

The only method that lags behind every other algorithm by a lot. Deepwalk remains always very close
to the best algorithm. In email-EU-core all algorithms other than deepwalk perform horribly
comparably. HARP which utilizes deepwalk has the best performance in the dblp dataset. It is
surprising that while Spectral Embedding is so old, in the PPI dataset outperforms all other
methods. The authors believe that this is because the PPI graphs have very small cuts that
correspond to the the cuts between clusters of interacting proteins.

As stated at the start of this report, no single algorithm can produce embeddings superior to all
others for all algorithms. Even a single algorithm is usually highly customizable with many
parameter choices whose optimums change depending on the input graph.


\spara{Graph Neural Network results}
Table \ref{table:2} is presenting the result of running the $3$ neural network
architectures proposed in Section  3. Namely, we use the \cite{kipf2016semi}
architecture to perform classification in a semi-supervised manners, using the
Chebyshev polynomial filters proposed by \cite{defferrard2016convolutional} for
different values of $K$, the $1^st$ approximation used in this paper. Also, we
compare against the supervised version of GraphSAGE. As we see, the
simplification proposed by \cite{kipf2016semi} seems to outperform the original
Chebyshev filters. GraphSAGE achieves the best accuracy results, however, it
uses only labeled training points.
\begin{table}[h!]
\centering
\caption{Prediction results for 3 citation datasets (average classification accuracy)}
\begin{tabular}{ |p{2cm}||p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
 \hline
 &\multicolumn{3}{c}{\textbf{Order of Chebyshev's Polynomial}} &&\\
 \hline
 &K=1& K=2 &K=3 &GCN & GraphSAGE\\
 \hline\hline
 Cora   & 0.7990    &0.84362 &0.793 &0.809 &\textbf{0.849}\\
 citeseer &   0.702  & 0.692 &0.68 &\textbf{0.715}& --- \\
 pubmed &0.71480 & 0.713 &0.74 &0.792 &\textbf{0.837}\\
 \hline
\end{tabular}\\
\label{table:2}
\end{table}

Table \ref{table:1} is performing supervised and unsupervised tasks on the PPI
dataset, using 2 different aggregation functions of GraphSAGE ($mean$ and
$max$). We also used an aggregation function that mimicks the convolutional
network of \cite{kipf2016semi}. As we see, graphSAGE outperforms this approach
on the PPI dataset, as well as the DeepWalk embedding technique.
\begin{table}[h!]
\centering
\caption{Prediction results for PPI dataset (micro-averaged F1 scores). \newline}
\begin{tabular}{|p{3cm}||p{3cm}|p{3cm}|}
 \hline
 \multicolumn{3}{|c|}{\textbf{PPI}} \\
 \hline
 & Supervised F1 &Unsupervised F1\\
 \hline\hline
 DeepWalk   & ---    &0.39\\
 max\_pool &   0.45131  & \textbf{0.47} \\
 mean\_pool &\textbf{0.51893} & 0.45\\
 GCN    &0.45438 & 0.44\\
 \hline
\end{tabular}\\
\label{table:1}
\end{table}

